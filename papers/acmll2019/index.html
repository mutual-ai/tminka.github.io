<html> <head>
<title>From automatic differentiation to message passing</title>
</head>

<body>
<h1>From automatic differentiation to message passing</h1>

Invited talk at the <a href="https://www.bigdata.cam.ac.uk/events/advances-and-challenges-machine-learning-languages">Advances and challenges in Machine Learning Languages workshop (ACMLL 2019)</a>
<br>
A shorter version appeared in <a href="http://www.autodiff.org/?module=Workshops&submenu=EuroAD%2F22%2Fprogramme">EuroAD 2019</a>

<p>
Automatic differentiation is an elegant technique for converting a computable function expressed as a program into a derivative-computing program with similar time complexity. It does not execute the original program as a black-box, nor does it expand the program into a mathematical formula, both of which would be counter-productive. By generalizing this technique, you can produce efficient algorithms for constraint satisfaction, optimization, and Bayesian inference on models specified as programs.  This approach can be broadly described as compiling into a message-passing program.
</p>

<a href="minka-acmll2019-slides.pdf">pdf slides</a>

<hr>
<address><a href="/">Tom Minka</a></address>
</body> </html>
